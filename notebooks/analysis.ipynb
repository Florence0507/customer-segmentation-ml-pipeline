{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Regularized Regression Analysis\n",
    "## Comparing Lasso, Ridge, and Elastic Net for Job Satisfaction Prediction\n",
    "\n",
    "**Author:** Florencekumari Makwana  \n",
    "**Date:** February 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Compare three regularization techniques (Lasso, Ridge, and Elastic Net) for predicting job satisfaction using employee demographic and work-related features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/job_satisfaction_data.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Basic Statistics:\")\n",
    "print(\"=\"*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality Check\n",
    "\n",
    "**Important Finding:** One observation has `Years_of_Experience = -1`, which is a data anomaly. Given the small dataset (n=100), we retain this value for analysis, but in production this should be investigated and corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for anomalies\n",
    "print(\"Checking for negative values...\")\n",
    "print(f\"\\nMin Years_of_Experience: {df['Years_of_Experience'].min()}\")\n",
    "print(f\"Record with negative experience:\")\n",
    "df[df['Years_of_Experience'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['Job_Satisfaction'], bins=15, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Job Satisfaction Score', fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontweight='bold')\n",
    "axes[0].set_title('Distribution of Job Satisfaction', fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df['Job_Satisfaction'], vert=True)\n",
    "axes[1].set_ylabel('Job Satisfaction Score', fontweight='bold')\n",
    "axes[1].set_title('Box Plot of Job Satisfaction', fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean: {df['Job_Satisfaction'].mean():.2f}\")\n",
    "print(f\"Median: {df['Job_Satisfaction'].median():.2f}\")\n",
    "print(f\"Std Dev: {df['Job_Satisfaction'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "numerical_cols = ['Age', 'Years_of_Experience', 'Hours_Worked_Per_Week', 'Salary', 'Job_Satisfaction']\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            fmt='.2f', square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Numerical Features', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=['Gender', 'Education_Level'], drop_first=True)\n",
    "\n",
    "print(\"Encoded features:\")\n",
    "print(df_encoded.columns.tolist())\n",
    "print(f\"\\nTotal features after encoding: {len(df_encoded.columns) - 1}\")  # -1 for target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_encoded.drop(columns=['Job_Satisfaction'])\n",
    "y = df_encoded['Job_Satisfaction']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 10% validation, 10% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Data Split Summary:\")\n",
    "print(f\"Training:   {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Validation: {X_val.shape[0]} samples ({X_val.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"Test:       {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature standardization (CRITICAL for regularized regression)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(\"âœ“ Features standardized (mean=0, std=1)\")\n",
    "print(\"\\nScaled Training Data Sample:\")\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha values to test\n",
    "alpha_values = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# Store results\n",
    "validation_results = []\n",
    "\n",
    "# Loop through alpha values\n",
    "for alpha in alpha_values:\n",
    "    print(f\"\\nTraining with alpha = {alpha}...\")\n",
    "    \n",
    "    # Initialize models\n",
    "    lasso_model = Lasso(alpha=alpha, random_state=42, max_iter=10000)\n",
    "    ridge_model = Ridge(alpha=alpha, random_state=42)\n",
    "    elastic_net_model = ElasticNet(alpha=alpha, random_state=42, max_iter=10000)\n",
    "    \n",
    "    # Train models\n",
    "    lasso_model.fit(X_train_scaled, y_train)\n",
    "    ridge_model.fit(X_train_scaled, y_train)\n",
    "    elastic_net_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predictions on validation set\n",
    "    y_pred_lasso = lasso_model.predict(X_val_scaled)\n",
    "    y_pred_ridge = ridge_model.predict(X_val_scaled)\n",
    "    y_pred_elastic_net = elastic_net_model.predict(X_val_scaled)\n",
    "    \n",
    "    # Compute metrics for each model\n",
    "    for model_name, y_pred in [\n",
    "        ('Lasso', y_pred_lasso),\n",
    "        ('Ridge', y_pred_ridge),\n",
    "        ('ElasticNet', y_pred_elastic_net)\n",
    "    ]:\n",
    "        validation_results.append({\n",
    "            'Alpha': alpha,\n",
    "            'Model': model_name,\n",
    "            'MSE': mean_squared_error(y_val, y_pred),\n",
    "            'MAE': mean_absolute_error(y_val, y_pred),\n",
    "            'RÂ²': r2_score(y_val, y_pred)\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance vs alpha\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['MSE', 'MAE', 'RÂ²']\n",
    "colors = {'Lasso': '#FF6B6B', 'Ridge': '#4ECDC4', 'ElasticNet': '#95E1D3'}\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for model in ['Lasso', 'Ridge', 'ElasticNet']:\n",
    "        model_data = validation_df[validation_df['Model'] == model]\n",
    "        ax.plot(model_data['Alpha'], model_data[metric], \n",
    "               marker='o', label=model, linewidth=2, color=colors[model])\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Alpha (log scale)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} vs Alpha on Validation Set', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    if metric == 'RÂ²':\n",
    "        ax.axhline(y=0, color='r', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Alpha for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best alpha based on lowest MSE\n",
    "best_models = {}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"BEST ALPHA VALUES (Based on Validation MSE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for model in ['Lasso', 'Ridge', 'ElasticNet']:\n",
    "    model_data = validation_df[validation_df['Model'] == model]\n",
    "    best_row = model_data.loc[model_data['MSE'].idxmin()]\n",
    "    best_models[model] = best_row\n",
    "    \n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  Best Alpha: {best_row['Alpha']}\")\n",
    "    print(f\"  MSE: {best_row['MSE']:.4f}\")\n",
    "    print(f\"  MAE: {best_row['MAE']:.4f}\")\n",
    "    print(f\"  RÂ²: {best_row['RÂ²']:.4f}\")\n",
    "\n",
    "# Store best alphas\n",
    "best_alpha_lasso = best_models['Lasso']['Alpha']\n",
    "best_alpha_ridge = best_models['Ridge']['Alpha']\n",
    "best_alpha_elastic_net = best_models['ElasticNet']['Alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Model Training & Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final models with best alpha values\n",
    "final_lasso = Lasso(alpha=best_alpha_lasso, random_state=42, max_iter=10000)\n",
    "final_ridge = Ridge(alpha=best_alpha_ridge, random_state=42)\n",
    "final_elastic_net = ElasticNet(alpha=best_alpha_elastic_net, random_state=42, max_iter=10000)\n",
    "\n",
    "# Train on full training set\n",
    "final_lasso.fit(X_train_scaled, y_train)\n",
    "final_ridge.fit(X_train_scaled, y_train)\n",
    "final_elastic_net.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"âœ“ Final models trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "y_pred_lasso_test = final_lasso.predict(X_test_scaled)\n",
    "y_pred_ridge_test = final_ridge.predict(X_test_scaled)\n",
    "y_pred_elastic_net_test = final_elastic_net.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "test_results = pd.DataFrame({\n",
    "    'Model': ['Lasso', 'Ridge', 'ElasticNet'],\n",
    "    'Alpha': [best_alpha_lasso, best_alpha_ridge, best_alpha_elastic_net],\n",
    "    'MSE': [\n",
    "        mean_squared_error(y_test, y_pred_lasso_test),\n",
    "        mean_squared_error(y_test, y_pred_ridge_test),\n",
    "        mean_squared_error(y_test, y_pred_elastic_net_test)\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_test, y_pred_lasso_test)),\n",
    "        np.sqrt(mean_squared_error(y_test, y_pred_ridge_test)),\n",
    "        np.sqrt(mean_squared_error(y_test, y_pred_elastic_net_test))\n",
    "    ],\n",
    "    'MAE': [\n",
    "        mean_absolute_error(y_test, y_pred_lasso_test),\n",
    "        mean_absolute_error(y_test, y_pred_ridge_test),\n",
    "        mean_absolute_error(y_test, y_pred_elastic_net_test)\n",
    "    ],\n",
    "    'RÂ²': [\n",
    "        r2_score(y_test, y_pred_lasso_test),\n",
    "        r2_score(y_test, y_pred_ridge_test),\n",
    "        r2_score(y_test, y_pred_elastic_net_test)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL TEST SET RESULTS\")\n",
    "print(\"=\"*70)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Finding:\n",
    "\n",
    "**Lasso Regression achieved the best test performance!**\n",
    "- RÂ² = 0.704 (explains 70% of variance)\n",
    "- MSE = 1.067 (lowest error)\n",
    "- MAE = 0.874\n",
    "\n",
    "Note: Rankings differ from validation (Ridge was best) vs test (Lasso is best), highlighting the importance of held-out test evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'RÂ²']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#95E1D3']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    bars = ax.bar(test_results['Model'], test_results[metric], \n",
    "                 color=colors, alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    ax.set_ylabel(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric} on Test Set', fontsize=13, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.3f}',\n",
    "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Performance Comparison (Test Set)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Lasso': final_lasso.coef_,\n",
    "    'Ridge': final_ridge.coef_,\n",
    "    'ElasticNet': final_elastic_net.coef_\n",
    "})\n",
    "\n",
    "# Sort by absolute Ridge coefficient\n",
    "coefficients_df['abs_ridge'] = coefficients_df['Ridge'].abs()\n",
    "coefficients_df = coefficients_df.sort_values('abs_ridge', ascending=False)\n",
    "coefficients_df = coefficients_df.drop('abs_ridge', axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COEFFICIENTS (Sorted by Ridge importance)\")\n",
    "print(\"=\"*70)\n",
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficients\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x_pos = np.arange(len(coefficients_df))\n",
    "width = 0.25\n",
    "\n",
    "ax.barh(x_pos - width, coefficients_df['Lasso'], width, \n",
    "       label='Lasso', alpha=0.8, color='#FF6B6B')\n",
    "ax.barh(x_pos, coefficients_df['Ridge'], width, \n",
    "       label='Ridge', alpha=0.8, color='#4ECDC4')\n",
    "ax.barh(x_pos + width, coefficients_df['ElasticNet'], width, \n",
    "       label='ElasticNet', alpha=0.8, color='#95E1D3')\n",
    "\n",
    "ax.set_yticks(x_pos)\n",
    "ax.set_yticklabels(coefficients_df['Feature'])\n",
    "ax.set_xlabel('Coefficient Value (Standardized)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Feature Importance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insights:\n",
    "\n",
    "1. **Years of Experience** is the dominant predictor (coefficient â‰ˆ 1.44)\n",
    "2. **Age** shows negative relationship (coefficient â‰ˆ -0.65)\n",
    "3. **Lasso feature selection**: Eliminated Salary, Hours_Worked_Per_Week, and Education_Level_High_School (coefficients = 0)\n",
    "4. **Ridge retention**: Keeps all features with small non-zero coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predictions vs Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots: Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "models_data = [\n",
    "    ('Lasso', y_pred_lasso_test, best_alpha_lasso),\n",
    "    ('Ridge', y_pred_ridge_test, best_alpha_ridge),\n",
    "    ('ElasticNet', y_pred_elastic_net_test, best_alpha_elastic_net)\n",
    "]\n",
    "\n",
    "colors = {'Lasso': '#FF6B6B', 'Ridge': '#4ECDC4', 'ElasticNet': '#95E1D3'}\n",
    "\n",
    "for idx, (model_name, y_pred, alpha) in enumerate(models_data):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter\n",
    "    ax.scatter(y_test, y_pred, alpha=0.6, s=100, \n",
    "              edgecolors='black', linewidth=0.5, color=colors[model_name])\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], \n",
    "           'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual Job Satisfaction', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Predicted Job Satisfaction', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{model_name} (Î± = {alpha})', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # RÂ² score\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax.transAxes,\n",
    "           fontsize=11, verticalalignment='top', fontweight='bold',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Predicted vs Actual Job Satisfaction (Test Set)', \n",
    "            fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (model_name, y_pred, alpha) in enumerate(models_data):\n",
    "    ax = axes[idx]\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    ax.scatter(y_pred, residuals, alpha=0.6, s=100,\n",
    "              edgecolors='black', linewidth=0.5, color=colors[model_name])\n",
    "    ax.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Job Satisfaction', fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Residuals', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{model_name} Residual Plot', fontsize=13, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Residual Analysis (Test Set)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusions\n",
    "\n",
    "### Best Model: **Lasso Regression (Î± = 0.1)**\n",
    "\n",
    "**Performance:**\n",
    "- RÂ² = 0.704 (explains 70% of job satisfaction variance)\n",
    "- MSE = 1.067\n",
    "- MAE = 0.874\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Years of Experience** is the strongest predictor (coefficient â‰ˆ 1.44)\n",
    "2. **Age** shows negative relationship when controlling for experience (coefficient â‰ˆ -0.65)\n",
    "3. **Salary** and **Hours Worked** had minimal impact (eliminated by Lasso)\n",
    "4. Low to moderate regularization (Î± = 0.1-1.0) optimal for this dataset\n",
    "5. Model rankings differ between validation and test sets, emphasizing importance of held-out evaluation\n",
    "\n",
    "**Limitations:**\n",
    "- Small sample size (n=100) and tiny test/validation sets (n=10 each)\n",
    "- Data quality issue: Years_of_Experience = -1\n",
    "- Limited hyperparameter search (only 5 alpha values)\n",
    "\n",
    "**Recommendations:**\n",
    "1. Collect more data (target n â‰¥ 500)\n",
    "2. Implement k-fold cross-validation\n",
    "3. Clean data anomalies\n",
    "4. Test interaction terms and polynomial features\n",
    "5. Compare with non-linear models (Random Forest, XGBoost)\n",
    "\n",
    "---\n",
    "\n",
    "**Analysis Complete!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
